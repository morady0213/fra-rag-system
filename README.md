# ğŸ›ï¸ FRA RAG System

**Retrieval-Augmented Generation System for the Egyptian Financial Regulatory Authority (FRA)**

Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ø²Ø² Ø¨Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ù„Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©

---

## Overview

A complete, modular Python RAG system designed specifically for Arabic legal and regulatory documents from [fra.gov.eg](https://fra.gov.eg). The system combines web scraping, document processing, semantic search, and LLM-powered answer generation.

### Key Features

- **Arabic-Optimized**: Strict Arabic text normalization (Alef unification, diacritics removal, etc.)
- **Hybrid Scraping**: Firecrawl for web content + Scrapy for PDF downloads
- **SOTA Embeddings**: BAAI/bge-m3 for multilingual semantic search
- **Legal Document Aware**: Arabic-specific chunking separators (Ø§Ù„Ù…Ø§Ø¯Ø©, Ù‚Ø±Ø§Ø±, etc.)
- **Grok Integration**: xAI's Grok for context-grounded answer generation

---

## Project Structure

```
fra-rag-system/
â”œâ”€â”€ config.py                 # Centralized configuration
â”œâ”€â”€ main.py                   # Main entry point
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env.example              # Environment variables template
â”‚
â”œâ”€â”€ scrapers/                 # Web scraping module
â”‚   â”œâ”€â”€ firecrawl_spider.py   # Firecrawl API for web content
â”‚   â””â”€â”€ pdf_spider.py         # Scrapy spider for PDF downloads
â”‚
â”œâ”€â”€ ingestion/                # Document processing module
â”‚   â”œâ”€â”€ arabic_utils.py       # Arabic text normalization
â”‚   â”œâ”€â”€ ocr_processor.py      # PDF parsing (PyMuPDF + PaddleOCR stub)
â”‚   â””â”€â”€ chunking.py           # Arabic-aware text chunking
â”‚
â”œâ”€â”€ rag_engine/               # Retrieval module
â”‚   â”œâ”€â”€ vector_store.py       # ChromaDB with BGE-M3 embeddings
â”‚   â””â”€â”€ retriever.py          # High-level retrieval interface
â”‚
â”œâ”€â”€ llm_client/               # LLM module
â”‚   â””â”€â”€ grok_client.py        # xAI Grok API client
â”‚
â””â”€â”€ data/                     # Data directories
    â”œâ”€â”€ sample_docs/          # Place documents here for ingestion
    â”œâ”€â”€ raw_pdfs/             # Downloaded PDFs from scraper
    â”œâ”€â”€ processed/            # Processed markdown files
    â””â”€â”€ chroma_db/            # Vector database (auto-created)
```

---

## Installation

### 1. Clone/Create the project

```bash
cd fra-rag-system
```

### 2. Create virtual environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/macOS
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment

```bash
# Copy example env file
cp .env.example .env

# Edit .env and add your API keys
# XAI_API_KEY=your_xai_api_key
# FIRECRAWL_API_KEY=your_firecrawl_api_key (optional)
```

---

## Usage

### Quick Start

```bash
# 1. Place documents in data/sample_docs/
# 2. Run the system
python main.py
```

### Command Line Options

```bash
# Interactive mode (default)
python main.py

# Force re-index documents
python main.py --ingest

# Single query mode
python main.py --query "Ù…Ø§ Ù‡ÙŠ Ø§Ø®ØªØµØ§ØµØ§Øª Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©ØŸ"

# Show statistics
python main.py --stats

# Custom number of retrieved documents
python main.py --query "Ø³Ø¤Ø§Ù„Ùƒ" --k 10
```

### Programmatic Usage

```python
from main import FRARAGSystem

# Initialize the system
system = FRARAGSystem()

# Ingest documents (if needed)
if not system.is_indexed():
    system.ingest_documents()

# Query the system
answer = system.query("Ù…Ø§ Ù‡ÙŠ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªØ±Ø®ÙŠØµØŸ")
print(answer)
```

---

## Module Documentation

### 1. Scrapers Module (`scrapers/`)

#### Firecrawl Spider

```python
from scrapers import FirecrawlScraper

scraper = FirecrawlScraper()

# Map website structure
urls = scraper.map_website()

# Crawl and extract content
pages = scraper.crawl_website(max_pages=50)

# Scrape specific sections
results = scraper.scrape_specific_sections()
```

#### PDF Spider (Scrapy)

```python
from scrapers import run_pdf_spider

# Run the spider to download PDFs
run_pdf_spider()
```

Or via command line:
```bash
cd scrapers
scrapy runspider pdf_spider.py
```

### 2. Ingestion Module (`ingestion/`)

#### Arabic Text Normalization

```python
from ingestion import normalize_text

text = "Ø§Ù„Ù‚ÙØ±ÙØ§Ø±Ù Ø±ÙÙ‚Ù’Ù… Ù¡Ù¢Ù£"
normalized = normalize_text(text)
# Output: "Ø§Ù„Ù‚Ø±Ø§Ø± Ø±Ù‚Ù… Ù¡Ù¢Ù£"
```

**Normalization rules:**
- Alef unification: `Ø£, Ø¥, Ø¢, Ù± â†’ Ø§`
- Yeh unification: `Ù‰ â†’ ÙŠ`
- Tatweel removal: `Ù€` removed
- Diacritics removal: All tashkeel removed
- Whitespace normalization

#### PDF Processing

```python
from ingestion import PDFProcessor

processor = PDFProcessor()

# Process a single PDF
doc = processor.process_file("document.pdf")
print(doc.full_text)

# Process all PDFs in a directory
docs = processor.process_directory("data/raw_pdfs/")
```

#### Text Chunking

```python
from ingestion import ArabicTextChunker

chunker = ArabicTextChunker(
    chunk_size=1000,
    chunk_overlap=200,
)

result = chunker.chunk_text(text, source="document.pdf")
for chunk in result.chunks:
    print(chunk.content)
```

**Arabic-aware separators:**
- `\n\n` - Paragraph breaks
- `\n` - Line breaks
- `Ø§Ù„Ù…Ø§Ø¯Ø©` - Article
- `Ù‚Ø±Ø§Ø±` - Decision
- `Ø§Ù„Ø¨Ù†Ø¯` - Clause
- `Ø§Ù„ÙØµÙ„` - Chapter

### 3. RAG Engine (`rag_engine/`)

#### Vector Store

```python
from rag_engine import VectorStore

store = VectorStore()

# Add documents
docs = [
    {"text": "Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©...", "metadata": {"source": "doc.pdf"}},
]
store.add_documents(docs)

# Search
results = store.search("Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«", k=5)
```

#### Retriever

```python
from rag_engine import Retriever

retriever = Retriever()

# Retrieve relevant documents
response = retriever.retrieve("Ù…Ø§ Ù‡ÙŠ Ø§Ø®ØªØµØ§ØµØ§Øª Ø§Ù„Ù‡ÙŠØ¦Ø©ØŸ", k=5)

for result in response.results:
    print(f"Source: {result.source}")
    print(f"Score: {result.score}")
    print(f"Content: {result.content[:200]}...")
```

### 4. LLM Client (`llm_client/`)

```python
from llm_client import GrokClient

client = GrokClient()

result = client.generate(
    query="Ù…Ø§ Ù‡ÙŠ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªØ±Ø®ÙŠØµØŸ",
    context="Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚...",
    sources=["doc1.pdf", "doc2.pdf"],
)

print(result.answer)
```

---

## Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `XAI_API_KEY` | xAI (Grok) API key | Yes |
| `FIRECRAWL_API_KEY` | Firecrawl API key | For scraping |
| `GROK_MODEL` | Model name (default: grok-beta) | No |
| `CHROMA_PERSIST_DIR` | Vector DB path | No |
| `EMBEDDING_MODEL` | Embedding model | No |

### Configuration File (`config.py`)

Key settings:
- `CHUNK_SIZE`: 1000 characters
- `CHUNK_OVERLAP`: 200 characters
- `DEFAULT_TOP_K`: 5 documents
- `ARABIC_SEPARATORS`: Legal document separators

---

## Arabic NLP Details

### Why Arabic Normalization Matters

Arabic text has many variations that should be treated as equivalent:
- **Alef variants**: Ø£, Ø¥, Ø¢ are all normalized to Ø§
- **Yeh/Alef Maksura**: Ù‰ is normalized to ÙŠ
- **Diacritics**: Short vowel marks are removed for matching
- **Tatweel**: Decorative elongation character is removed

### BGE-M3 for Arabic

We use `BAAI/bge-m3` because:
- State-of-the-art multilingual embeddings
- Excellent Arabic language support
- Supports up to 8192 tokens
- Dense, sparse, and multi-vector retrieval

---

## OCR Support (Optional)

For scanned PDFs, uncomment the PaddleOCR integration in `ocr_processor.py`:

```bash
# Install PaddleOCR
pip install paddlepaddle paddleocr
```

Then update `ocr_processor.py` to enable OCR.

---

## Troubleshooting

### "No documents found"

Place documents in `data/sample_docs/` or PDFs in `data/raw_pdfs/`.

### "LLM not available"

Set the `XAI_API_KEY` environment variable.

### Slow embedding generation

First run downloads the BGE-M3 model (~2GB). Subsequent runs use cached model.

### Memory issues

Reduce `CHUNK_SIZE` or process fewer documents at once.

---

## License

MIT License

---

## Acknowledgments

- [Firecrawl](https://firecrawl.dev/) - Web scraping API
- [ChromaDB](https://www.trychroma.com/) - Vector database
- [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) - Embedding model
- [xAI Grok](https://x.ai/) - LLM API
- [PyMuPDF](https://pymupdf.readthedocs.io/) - PDF processing
