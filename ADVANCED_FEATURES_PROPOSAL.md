# FRA RAG System - Advanced Features Proposal

## Executive Summary

This proposal outlines a phased approach to evolve the FRA RAG system from a solid retrieval-augmented generation platform into an enterprise-grade **Legal AI Assistant** with advanced reasoning, knowledge graphs, and rigorous evaluation capabilities.

---

## Part 1: Technical Advancements

### 1. Hierarchical Document Chunking (Parent-Document Retrieval)

#### Current Limitation
```
Chunk 1: "Ø§Ù„Ù…Ø§Ø¯Ø© 5: ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ©..."
Chunk 2: "...Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ù…Ù† Ø°Ù„Ùƒ ÙÙŠ Ø­Ø§Ù„Ø©..."  â† Context lost!
```

#### Proposed Architecture: Small-to-Big Retrieval

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Document: Ù‚Ø§Ù†ÙˆÙ†_Ø§Ù„ØªÙ…ÙˆÙŠÙ„.docx                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              Chapter: Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø£ÙˆÙ„ - Ø§Ù„ØªØ±Ø§Ø®ÙŠØµ                â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚  â”‚  â”‚           Article: Ø§Ù„Ù…Ø§Ø¯Ø© 5 - Ù…ØªØ·Ù„Ø¨Ø§Øª Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„         â”‚â”‚â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚â”‚
â”‚  â”‚  â”‚  â”‚ Clause 1      â”‚ â”‚ Clause 2      â”‚ â”‚ Exception     â”‚ â”‚â”‚â”‚
â”‚  â”‚  â”‚  â”‚ (Indexed)     â”‚ â”‚ (Indexed)     â”‚ â”‚ (Indexed)     â”‚ â”‚â”‚â”‚
â”‚  â”‚  â”‚  â”‚ Vector: âœ“     â”‚ â”‚ Vector: âœ“     â”‚ â”‚ Vector: âœ“     â”‚ â”‚â”‚â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚â”‚
â”‚  â”‚  â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚â”‚â”‚
â”‚  â”‚  â”‚                            â–¼                            â”‚â”‚â”‚
â”‚  â”‚  â”‚               Parent Reference: article_5_id            â”‚â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Implementation Plan

**Phase 1: Hierarchical Parser**

```python
# New file: ingestion/hierarchical_chunker.py

@dataclass
class DocumentNode:
    """Hierarchical document node."""
    id: str
    content: str
    level: str  # "document" | "chapter" | "article" | "clause"
    parent_id: Optional[str]
    children_ids: List[str]
    metadata: Dict[str, Any]

class HierarchicalChunker:
    """
    Parse legal documents into hierarchical structure.
    
    Detection patterns:
    - Ø§Ù„Ø¨Ø§Ø¨ / Ø§Ù„ÙØµÙ„ â†’ Chapter
    - Ø§Ù„Ù…Ø§Ø¯Ø© â†’ Article
    - Ø§Ù„Ø¨Ù†Ø¯ / Ø§Ù„ÙÙ‚Ø±Ø© â†’ Clause
    - Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø«Ø§Ù†ÙŠØ§Ù‹ â†’ Numbered items
    """
    
    HIERARCHY_PATTERNS = {
        "chapter": [r"Ø§Ù„Ø¨Ø§Ø¨\s+(\w+)", r"Ø§Ù„ÙØµÙ„\s+(\w+)"],
        "article": [r"Ù…Ø§Ø¯Ø©\s*\(?\s*(\d+)\s*\)?", r"Ø§Ù„Ù…Ø§Ø¯Ø©\s+(\d+)"],
        "clause": [r"Ø§Ù„Ø¨Ù†Ø¯\s+(\d+)", r"Ø§Ù„ÙÙ‚Ø±Ø©\s+(\w+)"],
        "item": [r"(Ø£ÙˆÙ„Ø§Ù‹|Ø«Ø§Ù†ÙŠØ§Ù‹|Ø«Ø§Ù„Ø«Ø§Ù‹|Ø±Ø§Ø¨Ø¹Ø§Ù‹|Ø®Ø§Ù…Ø³Ø§Ù‹)"],
    }
    
    def parse(self, text: str, source: str) -> List[DocumentNode]:
        """Parse document into hierarchical nodes."""
        # 1. Detect structure markers
        # 2. Build tree
        # 3. Return flattened nodes with parent references
```

**Phase 2: Dual-Index Storage**

```python
# Store in Qdrant with parent references

# Small chunks (for precise retrieval)
small_chunk = {
    "id": "clause_5_1_uuid",
    "text": "Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ 50 Ù…Ù„ÙŠÙˆÙ† Ø¬Ù†ÙŠÙ‡",
    "level": "clause",
    "parent_id": "article_5_uuid",
    "grandparent_id": "chapter_1_uuid",
    "root_id": "document_uuid",
}

# Parent documents (for context expansion)
parent_doc = {
    "id": "article_5_uuid",
    "text": "Ø§Ù„Ù…Ø§Ø¯Ø© 5 - Ù…ØªØ·Ù„Ø¨Ø§Øª Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„\n1. Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰...\n2. Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª...",
    "level": "article",
    "children_ids": ["clause_5_1_uuid", "clause_5_2_uuid"],
}
```

**Phase 3: Smart Retrieval**

```python
class ParentDocumentRetriever:
    """
    Retrieve small chunks, expand to parent context.
    """
    
    def retrieve(self, query: str, k: int = 5) -> List[Dict]:
        # 1. Search small chunks (high precision)
        small_results = self.vector_store.search(
            query, k=k*2, filter={"level": ["clause", "item"]}
        )
        
        # 2. Get unique parent articles
        parent_ids = set(r["parent_id"] for r in small_results)
        
        # 3. Fetch full parent content
        parents = self.vector_store.get_by_ids(list(parent_ids))
        
        # 4. Return parents with matched clause highlighted
        return self._merge_with_highlights(parents, small_results)
```

#### Benefits
- âœ… High precision search on specific clauses
- âœ… Full article context for LLM
- âœ… Preserves legal document structure
- âœ… Enables "show me the full article" feature

#### Effort Estimate
- **Development**: 2-3 weeks
- **Testing**: 1 week
- **Priority**: **HIGH** (foundational improvement)

---

### 2. Semantic Metadata Extraction

#### Current Limitation
- No filtering by date, entity type, or document status
- Cannot answer "What applies to Banks in 2024?"

#### Proposed Architecture

```
Document Ingestion
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Metadata Extraction (Small LLM)         â”‚
â”‚                                                   â”‚
â”‚  Input: Document text                            â”‚
â”‚  Output: {                                       â”‚
â”‚    "law_status": "active",                       â”‚
â”‚    "effective_date": "2024-01-15",               â”‚
â”‚    "issuing_authority": "FRA",                   â”‚
â”‚    "entity_types": ["Bank", "Microfinance"],     â”‚
â”‚    "document_type": "regulation",                â”‚
â”‚    "topics": ["licensing", "capital", "branches"]â”‚
â”‚  }                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Qdrant Storage                       â”‚
â”‚                                                   â”‚
â”‚  payload: {                                      â”‚
â”‚    "text": "...",                                â”‚
â”‚    "metadata": {                                 â”‚
â”‚      "law_status": "active",                     â”‚
â”‚      "effective_date": "2024-01-15",             â”‚
â”‚      "entity_types": ["Bank", "Microfinance"],   â”‚
â”‚      ...                                         â”‚
â”‚    }                                             â”‚
â”‚  }                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Implementation

```python
# ingestion/metadata_extractor.py

class MetadataExtractor:
    """Extract semantic metadata using LLM."""
    
    EXTRACTION_PROMPT = """
    Ø­Ù„Ù„ Ø§Ù„Ù†Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©:
    
    Ø§Ù„Ù†Øµ: {text}
    
    Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨ØµÙŠØºØ© JSON:
    {{
        "law_status": "active" Ø£Ùˆ "repealed" Ø£Ùˆ "amended",
        "effective_date": "YYYY-MM-DD" Ø£Ùˆ null,
        "amendment_date": "YYYY-MM-DD" Ø£Ùˆ null,
        "issuing_authority": "Ø§Ø³Ù… Ø§Ù„Ø¬Ù‡Ø© Ø§Ù„Ù…ØµØ¯Ø±Ø©",
        "entity_types": ["Ù‚Ø§Ø¦Ù…Ø© Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ø®Ø§Ø¶Ø¹Ø©"],
        "document_type": "regulation" Ø£Ùˆ "decision" Ø£Ùˆ "circular" Ø£Ùˆ "form",
        "topics": ["Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"],
        "penalties_mentioned": true Ø£Ùˆ false,
        "capital_requirements_mentioned": true Ø£Ùˆ false
    }}
    """
    
    def extract(self, text: str) -> Dict[str, Any]:
        # Use small/fast LLM for extraction
        response = self.llm.generate(
            self.EXTRACTION_PROMPT.format(text=text[:4000])
        )
        return json.loads(response)
```

#### UI Filter Components

```python
# In app.py - Add filter dropdowns

with gr.Row():
    entity_filter = gr.Dropdown(
        choices=["Ø§Ù„ÙƒÙ„", "Ø¨Ù†ÙˆÙƒ", "ØªÙ…ÙˆÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§ÙƒÙŠ", "ØªÙ…ÙˆÙŠÙ„ Ù…ØªÙ†Ø§Ù‡ÙŠ Ø§Ù„ØµØºØ±", "Ø³Ù…Ø³Ø±Ø©", "ØªØ£Ù…ÙŠÙ†"],
        value="Ø§Ù„ÙƒÙ„",
        label="ðŸ¢ Ù†ÙˆØ¹ Ø§Ù„Ø¬Ù‡Ø© (Entity Type)"
    )
    doc_type_filter = gr.Dropdown(
        choices=["Ø§Ù„ÙƒÙ„", "Ù„Ø§Ø¦Ø­Ø©", "Ù‚Ø±Ø§Ø±", "ØªØ¹Ù…ÙŠÙ…", "Ù†Ù…ÙˆØ°Ø¬"],
        value="Ø§Ù„ÙƒÙ„",
        label="ðŸ“„ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ (Document Type)"
    )
    date_filter = gr.Dropdown(
        choices=["Ø§Ù„ÙƒÙ„", "2024", "2023", "2022", "2021", "Ù‚Ø¨Ù„ 2021"],
        value="Ø§Ù„ÙƒÙ„",
        label="ðŸ“… Ø§Ù„Ø³Ù†Ø© (Year)"
    )
```

#### Benefits
- âœ… Precise filtering by entity type
- âœ… Date-based queries ("What changed in 2024?")
- âœ… Document type filtering (forms vs regulations)
- âœ… Better retrieval precision

#### Effort Estimate
- **Development**: 1-2 weeks
- **Testing**: 1 week
- **Priority**: **HIGH** (addresses key user need)

---

### 3. Knowledge Graph / GraphRAG

#### Current Limitation
- Vector search misses conceptually related but semantically different content
- Cannot answer "What are ALL obligations of Microfinance companies?"

#### Proposed Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Knowledge Graph (Neo4j/NetworkX)              â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         REQUIRES          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Microfinanceâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ License         â”‚  â”‚
â”‚  â”‚ Company     â”‚                           â”‚                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                           â”‚           â”‚
â”‚         â”‚ MUST_HAVE                                 â”‚ NEEDS     â”‚
â”‚         â–¼                                           â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Capital     â”‚                           â”‚ Documents       â”‚  â”‚
â”‚  â”‚ 50M EGP     â”‚                           â”‚ - Application   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚ - Articles      â”‚  â”‚
â”‚         â”‚                                  â”‚ - Security Checkâ”‚  â”‚
â”‚         â”‚ EXCEPTION_IF                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚ Government  â”‚                                               â”‚
â”‚  â”‚ Ownership   â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Implementation

**Phase 1: Graph Schema Definition**

```python
# rag_engine/knowledge_graph.py

from dataclasses import dataclass
from typing import List, Optional
from enum import Enum

class EntityType(Enum):
    ENTITY = "entity"           # Bank, Microfinance, Insurance
    REQUIREMENT = "requirement" # License, Capital, Documents
    DOCUMENT = "document"       # Forms, Regulations
    CONDITION = "condition"     # Exceptions, Prerequisites
    PENALTY = "penalty"         # Fines, Revocations

class RelationType(Enum):
    REQUIRES = "REQUIRES"
    MUST_HAVE = "MUST_HAVE"
    NEEDS_DOCUMENT = "NEEDS_DOCUMENT"
    EXCEPTION_IF = "EXCEPTION_IF"
    SUBJECT_TO = "SUBJECT_TO"
    DEFINED_IN = "DEFINED_IN"
    AMENDS = "AMENDS"
    REPEALS = "REPEALS"

@dataclass
class GraphNode:
    id: str
    type: EntityType
    name: str
    name_ar: str
    attributes: Dict[str, Any]
    source_chunks: List[str]  # Links to vector store

@dataclass
class GraphEdge:
    source_id: str
    target_id: str
    relation: RelationType
    attributes: Dict[str, Any]
    source_chunk: str  # Where this relation was found
```

**Phase 2: Automatic Graph Construction**

```python
class GraphBuilder:
    """Build knowledge graph from document chunks."""
    
    EXTRACTION_PROMPT = """
    Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ø§Ù„ØªØ§Ù„ÙŠØŒ Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª:
    
    Ø§Ù„Ù†Øµ: {text}
    
    Ø§Ø³ØªØ®Ø±Ø¬ Ø¨ØµÙŠØºØ© JSON:
    {{
        "entities": [
            {{"name": "...", "type": "entity|requirement|document|condition|penalty", "attributes": {{}}}}
        ],
        "relations": [
            {{"source": "...", "target": "...", "relation": "REQUIRES|MUST_HAVE|EXCEPTION_IF|..."}}
        ]
    }}
    
    Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
    - REQUIRES: X ÙŠØªØ·Ù„Ø¨ Y
    - MUST_HAVE: X ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ…ØªÙ„Ùƒ Y
    - NEEDS_DOCUMENT: X ÙŠØ­ØªØ§Ø¬ Ù…Ø³ØªÙ†Ø¯ Y
    - EXCEPTION_IF: Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ù…Ù† X Ø¥Ø°Ø§ ØªØ­Ù‚Ù‚ Ø´Ø±Ø· Y
    - SUBJECT_TO: X Ø®Ø§Ø¶Ø¹ Ù„Ù€ Y
    """
    
    def build_from_chunks(self, chunks: List[Dict]) -> Tuple[List[GraphNode], List[GraphEdge]]:
        """Extract entities and relations from all chunks."""
        all_nodes = []
        all_edges = []
        
        for chunk in chunks:
            extracted = self.llm.generate(
                self.EXTRACTION_PROMPT.format(text=chunk["text"])
            )
            nodes, edges = self._parse_extraction(extracted, chunk["id"])
            all_nodes.extend(nodes)
            all_edges.extend(edges)
        
        # Deduplicate and merge nodes
        return self._merge_nodes(all_nodes), all_edges
```

**Phase 3: Graph-Enhanced Retrieval**

```python
class GraphRAGRetriever:
    """Combine vector search with graph traversal."""
    
    def retrieve(self, query: str, k: int = 5) -> Dict[str, Any]:
        # 1. Vector search for initial nodes
        vector_results = self.vector_store.search(query, k=k)
        
        # 2. Extract entities from query
        query_entities = self.extract_entities(query)
        
        # 3. Graph traversal from matched entities
        graph_context = []
        for entity in query_entities:
            # Find related nodes (1-2 hops)
            related = self.graph.traverse(
                start=entity,
                max_hops=2,
                relations=["REQUIRES", "MUST_HAVE", "EXCEPTION_IF"]
            )
            graph_context.extend(related)
        
        # 4. Fetch source chunks for graph nodes
        graph_chunks = self.vector_store.get_by_ids(
            [node.source_chunks[0] for node in graph_context]
        )
        
        # 5. Merge and deduplicate
        return self._merge_results(vector_results, graph_chunks)
```

#### Example Query Flow

```
Query: "Ù…Ø§ Ù‡ÙŠ Ø§Ù„ØªØ²Ø§Ù…Ø§Øª Ø´Ø±ÙƒØ§Øª Ø§Ù„ØªÙ…ÙˆÙŠÙ„ Ù…ØªÙ†Ø§Ù‡ÙŠ Ø§Ù„ØµØºØ±ØŸ"

1. Vector Search â†’ Finds chunks mentioning "Ø§Ù„ØªÙ…ÙˆÙŠÙ„ Ù…ØªÙ†Ø§Ù‡ÙŠ Ø§Ù„ØµØºØ±"

2. Entity Extraction â†’ Identifies: "Ø´Ø±ÙƒØ© Ø§Ù„ØªÙ…ÙˆÙŠÙ„ Ù…ØªÙ†Ø§Ù‡ÙŠ Ø§Ù„ØµØºØ±"

3. Graph Traversal:
   Microfinance Company
   â”œâ”€â”€ REQUIRES â†’ License (source: article_1)
   â”œâ”€â”€ MUST_HAVE â†’ Capital 50M (source: article_5)
   â”œâ”€â”€ MUST_HAVE â†’ Branch Manager (source: article_10)
   â”œâ”€â”€ SUBJECT_TO â†’ Annual Audit (source: article_15)
   â””â”€â”€ EXCEPTION_IF â†’ Government Ownership (source: article_5_exception)

4. Fetch ALL related chunks â†’ Complete context

5. LLM generates comprehensive answer with ALL obligations
```

#### Benefits
- âœ… Discovers related concepts not found by vector search
- âœ… Handles "global" questions (summarize all X)
- âœ… Explicit relationship tracking
- âœ… Explainable retrieval path

#### Effort Estimate
- **Development**: 4-6 weeks
- **Testing**: 2 weeks
- **Priority**: **MEDIUM** (high value but complex)

---

### 4. Agentic Workflow (ReAct Agent)

#### Current Limitation
- Single retrieval step cannot handle multi-hop reasoning
- Cannot answer: "Does Company X qualify for License Y given condition Z?"

#### Proposed Architecture: ReAct Agent

```
User Query: "Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø´Ø±ÙƒØ© ØªØ£Ù…ÙŠÙ† Ø¨Ø±Ø£Ø³ Ù…Ø§Ù„ 30 Ù…Ù„ÙŠÙˆÙ† Ø¬Ù†ÙŠÙ‡ Ø¥Ù„Ù‰ Ù…ÙˆØ§ÙÙ‚Ø© Ù…Ø³Ø¨Ù‚Ø©ØŸ"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ReAct Agent Loop                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚ Iteration 1:                                                    â”‚
â”‚   Thought: Ø£Ø­ØªØ§Ø¬ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„ØªØ£Ù…ÙŠÙ†    â”‚
â”‚   Action: retrieve("Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ø´Ø±ÙƒØ§Øª Ø§Ù„ØªØ£Ù…ÙŠÙ†")       â”‚
â”‚   Observation: "Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ 60 Ù…Ù„ÙŠÙˆÙ† Ø¬Ù†ÙŠÙ‡ (Ø§Ù„Ù…Ø§Ø¯Ø© 5)"           â”‚
â”‚                                                                  â”‚
â”‚ Iteration 2:                                                    â”‚
â”‚   Thought: Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ 30 Ù…Ù„ÙŠÙˆÙ† Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ 60 Ù…Ù„ÙŠÙˆÙ†ØŒ      â”‚
â”‚            Ø£Ø­ØªØ§Ø¬ Ù…Ø¹Ø±ÙØ© Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª                        â”‚
â”‚   Action: retrieve("Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ø´Ø±ÙƒØ§Øª Ø§Ù„ØªØ£Ù…ÙŠÙ†")         â”‚
â”‚   Observation: "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰"                  â”‚
â”‚                                                                  â”‚
â”‚ Iteration 3:                                                    â”‚
â”‚   Thought: Ù„Ø¯ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©                           â”‚
â”‚   Action: answer()                                              â”‚
â”‚                                                                  â”‚
â”‚ Final Answer:                                                   â”‚
â”‚   "Ù„Ø§ØŒ Ø´Ø±ÙƒØ© Ø§Ù„ØªØ£Ù…ÙŠÙ† Ø¨Ø±Ø£Ø³ Ù…Ø§Ù„ 30 Ù…Ù„ÙŠÙˆÙ† Ø¬Ù†ÙŠÙ‡ Ù„Ø§ ØªØ³ØªÙˆÙÙŠ Ø§Ù„Ø­Ø¯       â”‚
â”‚    Ø§Ù„Ø£Ø¯Ù†Ù‰ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙˆÙ‡Ùˆ 60 Ù…Ù„ÙŠÙˆÙ† Ø¬Ù†ÙŠÙ‡ØŒ ÙˆÙ„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª."      â”‚
â”‚                                                                  â”‚
â”‚   ðŸ“Œ Ø§Ù„Ù…ØµØ§Ø¯Ø±:                                                   â”‚
â”‚   - Ø§Ù„Ù…Ø§Ø¯Ø© 5: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„                           â”‚
â”‚   - Ø§Ù„Ù…Ø§Ø¯Ø© 5 ÙÙ‚Ø±Ø© 3: Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Implementation

```python
# rag_engine/react_agent.py

from enum import Enum
from typing import List, Tuple, Optional

class ActionType(Enum):
    RETRIEVE = "retrieve"
    CALCULATE = "calculate"
    COMPARE = "compare"
    ANSWER = "answer"

@dataclass
class AgentStep:
    thought: str
    action: ActionType
    action_input: str
    observation: str

class ReActAgent:
    """
    ReAct (Reasoning + Acting) agent for multi-hop legal reasoning.
    """
    
    SYSTEM_PROMPT = """
    Ø£Ù†Øª ÙˆÙƒÙŠÙ„ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠØ©.
    
    Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„ØŒ Ø§ØªØ¨Ø¹ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…Ø·:
    
    Thought: [ØªØ­Ù„ÙŠÙ„Ùƒ Ù„Ù„Ù…ÙˆÙ‚Ù ÙˆÙ…Ø§ ØªØ­ØªØ§Ø¬ Ù…Ø¹Ø±ÙØªÙ‡]
    Action: [retrieve|calculate|compare|answer]
    Action Input: [Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø£Ùˆ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª]
    
    Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
    - retrieve: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
    - calculate: Ø­Ø³Ø§Ø¨ Ø±ÙŠØ§Ø¶ÙŠ Ø¨Ø³ÙŠØ·
    - compare: Ù…Ù‚Ø§Ø±Ù†Ø© Ù‚ÙŠÙ…ØªÙŠÙ†
    - answer: ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (ÙÙ‚Ø· Ø¹Ù†Ø¯Ù…Ø§ ØªÙ…Ù„Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©)
    
    Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªÙÙƒÙŠØ± ÙˆØ§Ù„Ø¨Ø­Ø« Ø­ØªÙ‰ ØªØ¬Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©.
    Ù„Ø§ ØªØ®Ù…Ù† - Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ Ù‚Ù„ Ø°Ù„Ùƒ.
    """
    
    def __init__(self, retriever, llm, max_iterations: int = 5):
        self.retriever = retriever
        self.llm = llm
        self.max_iterations = max_iterations
    
    def run(self, query: str) -> Tuple[str, List[AgentStep]]:
        """Execute agent loop."""
        steps = []
        context = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {query}\n\n"
        
        for i in range(self.max_iterations):
            # Get next action from LLM
            response = self.llm.generate(
                self.SYSTEM_PROMPT + context
            )
            
            # Parse thought, action, action_input
            thought, action, action_input = self._parse_response(response)
            
            if action == ActionType.ANSWER:
                # Final answer
                return action_input, steps
            
            # Execute action
            observation = self._execute_action(action, action_input)
            
            # Record step
            step = AgentStep(thought, action, action_input, observation)
            steps.append(step)
            
            # Update context
            context += f"""
Thought: {thought}
Action: {action.value}
Action Input: {action_input}
Observation: {observation}

"""
        
        # Max iterations reached
        return self._synthesize_answer(query, steps), steps
    
    def _execute_action(self, action: ActionType, input: str) -> str:
        if action == ActionType.RETRIEVE:
            results = self.retriever.retrieve(input, k=3)
            return self._format_retrieval(results)
        elif action == ActionType.CALCULATE:
            return self._safe_calculate(input)
        elif action == ActionType.COMPARE:
            return self._compare_values(input)
        return ""
```

#### Benefits
- âœ… Multi-hop reasoning for complex questions
- âœ… Step-by-step transparency
- âœ… Reduces hallucination through verification
- âœ… Handles conditional logic

#### Effort Estimate
- **Development**: 2-3 weeks
- **Testing**: 2 weeks
- **Priority**: **HIGH** (significant capability upgrade)

---

### 5. Clickable Sources with Highlighting

#### Current Limitation
- Sources are displayed but not interactive
- User cannot quickly verify the cited text

#### Proposed UI Enhancement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Answer:                                                         â”‚
â”‚                                                                  â”‚
â”‚  Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ù‡Ùˆ 50 Ù…Ù„ÙŠÙˆÙ† Ø¬Ù†ÙŠÙ‡ [1].                    â”‚
â”‚  ÙŠØ¬Ø¨ ØªÙ‚Ø¯ÙŠÙ… Ø·Ù„Ø¨ Ø§Ù„ØªØ±Ø®ÙŠØµ Ø¥Ù„Ù‰ Ø§Ù„Ù‡ÙŠØ¦Ø© [2].                          â”‚
â”‚                                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                  â”‚
â”‚  [1] â† Clickable                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ðŸ“„ Ù†Ù…ÙˆØ°Ø¬-Ù‚ÙŠØ¯-ÙØ±Ø¹-Ù„Ø´Ø±ÙƒØ©-ØªÙ…ÙˆÙŠÙ„-Ø§Ù„Ø§Ø³ØªÙ‡Ù„Ø§ÙƒÙ‰.docx             â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚ Ø§Ù„Ù…Ø§Ø¯Ø© 5:                                                â”‚   â”‚
â”‚  â”‚ "ÙŠØ´ØªØ±Ø· Ù„Ù‚ÙŠØ¯ ÙØ±Ø¹ Ø´Ø±ÙƒØ© Ø§Ù„ØªÙ…ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ‡Ù„Ø§ÙƒÙŠ Ø£Ù† ÙŠÙƒÙˆÙ† Ø±Ø£Ø³    â”‚   â”‚
â”‚  â”‚  Ø§Ù„Ù…Ø§Ù„ Ø§Ù„Ù…Ø¯ÙÙˆØ¹ Ù„Ø§ ÙŠÙ‚Ù„ Ø¹Ù† â–ˆâ–ˆ 50 Ù…Ù„ÙŠÙˆÙ† Ø¬Ù†ÙŠÙ‡ Ù…ØµØ±ÙŠ â–ˆâ–ˆ"      â”‚   â”‚
â”‚  â”‚                         â†‘ Highlighted match              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Implementation

```python
# In app.py

def create_source_html(sources: List[Dict], answer: str) -> str:
    """Create HTML with clickable source references."""
    
    # Extract citation numbers from answer
    citations = re.findall(r'\[(\d+)\]', answer)
    
    html_parts = ['<div class="sources-container">']
    
    for i, source in enumerate(sources, 1):
        # Find the matched text in the answer
        matched_text = find_matched_phrase(answer, source["content"])
        
        # Highlight matched text in source
        highlighted_content = highlight_text(
            source["content"],
            matched_text,
            highlight_class="bg-yellow-200"
        )
        
        html_parts.append(f'''
        <div class="source-card" id="source-{i}">
            <div class="source-header" onclick="toggleSource({i})">
                <span class="source-number">[{i}]</span>
                <span class="source-name">ðŸ“„ {source["source"]}</span>
                <span class="relevance-badge">{source["score"]*100:.0f}%</span>
            </div>
            <div class="source-content" id="source-content-{i}" style="display:none;">
                <div class="source-text" dir="rtl">
                    {highlighted_content}
                </div>
            </div>
        </div>
        ''')
    
    html_parts.append('</div>')
    
    # Add JavaScript for interactivity
    html_parts.append('''
    <script>
    function toggleSource(id) {
        const content = document.getElementById('source-content-' + id);
        content.style.display = content.style.display === 'none' ? 'block' : 'none';
    }
    
    function scrollToSource(id) {
        const element = document.getElementById('source-' + id);
        element.scrollIntoView({ behavior: 'smooth' });
        toggleSource(id);
    }
    </script>
    ''')
    
    return '\n'.join(html_parts)

def format_answer_with_clickable_refs(answer: str) -> str:
    """Make citation numbers clickable."""
    return re.sub(
        r'\[(\d+)\]',
        r'<a href="#" onclick="scrollToSource(\1); return false;" class="citation-link">[\1]</a>',
        answer
    )
```

#### Benefits
- âœ… Quick verification of sources
- âœ… Highlighted matched text
- âœ… Better user experience
- âœ… Increased trust in system

#### Effort Estimate
- **Development**: 1 week
- **Testing**: 2-3 days
- **Priority**: **HIGH** (easy win, high impact)

---

## Part 2: Evaluation Framework

### 1. Golden Dataset Creation

#### Dataset Structure

```json
{
    "id": "q_001",
    "question": "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ù„Ø´Ø±ÙƒØ© ØªÙ…ÙˆÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§ÙƒÙŠØŸ",
    "question_type": "factual",
    "language": "ar",
    "ground_truth_answer": "Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ù‡Ùˆ 50 Ù…Ù„ÙŠÙˆÙ† Ø¬Ù†ÙŠÙ‡ Ù…ØµØ±ÙŠ.",
    "relevant_chunks": ["chunk_id_1", "chunk_id_2"],
    "relevant_docs": ["Ù†Ù…ÙˆØ°Ø¬-Ù‚ÙŠØ¯-ÙØ±Ø¹-Ù„Ø´Ø±ÙƒØ©-ØªÙ…ÙˆÙŠÙ„-Ø§Ù„Ø§Ø³ØªÙ‡Ù„Ø§ÙƒÙ‰.docx"],
    "difficulty": "easy",
    "requires_multi_hop": false,
    "metadata": {
        "entity_type": "consumer_finance",
        "topic": "capital_requirements"
    }
}
```

#### Question Types

| Type | Description | Example |
|------|-------------|---------|
| **factual** | Single fact retrieval | Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰...ØŸ |
| **comparison** | Compare two concepts | Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† X Ùˆ YØŸ |
| **procedural** | Steps/process | Ù…Ø§ Ù‡ÙŠ Ø®Ø·ÙˆØ§Øª...ØŸ |
| **conditional** | If-then logic | Ù‡Ù„ ÙŠØ¬ÙˆØ² X Ø¥Ø°Ø§ ÙƒØ§Ù† YØŸ |
| **aggregation** | Summarize multiple items | Ù…Ø§ Ù‡ÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ²Ø§Ù…Ø§Øª...ØŸ |
| **temporal** | Date-based | Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØºÙŠØ± ÙÙŠ 2024ØŸ |
| **negation** | What is NOT allowed | Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ø­Ø¸ÙˆØ±Ø§ØªØŸ |

#### Dataset Generation Strategy

```python
# evaluation/dataset_generator.py

class GoldenDatasetGenerator:
    """Generate evaluation dataset from documents."""
    
    QUESTION_GENERATION_PROMPT = """
    Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ø¦Ù„Ø© ØªÙ‚ÙŠÙŠÙ… Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.
    
    Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠØŒ Ø£Ù†Ø´Ø¦ {n} Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø©:
    
    Ø§Ù„Ù†Øµ: {text}
    
    Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„ØŒ Ù‚Ø¯Ù…:
    1. Ø§Ù„Ø³Ø¤Ø§Ù„
    2. Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ (factual/comparison/procedural/conditional/aggregation)
    3. Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©
    4. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø© (easy/medium/hard)
    5. Ù‡Ù„ ÙŠØªØ·Ù„Ø¨ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª (true/false)
    
    ØªØ£ÙƒØ¯ Ù…Ù† ØªÙ†ÙˆÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.
    """
    
    def generate_from_chunks(self, chunks: List[Dict], questions_per_chunk: int = 3) -> List[Dict]:
        """Generate questions from document chunks."""
        dataset = []
        
        for chunk in chunks:
            questions = self.llm.generate(
                self.QUESTION_GENERATION_PROMPT.format(
                    text=chunk["text"],
                    n=questions_per_chunk
                )
            )
            
            for q in questions:
                q["relevant_chunks"] = [chunk["id"]]
                q["relevant_docs"] = [chunk["source"]]
                dataset.append(q)
        
        return dataset
    
    def generate_comparison_questions(self, entity_pairs: List[Tuple[str, str]]) -> List[Dict]:
        """Generate comparison questions for entity pairs."""
        # e.g., ("ØªÙ…ÙˆÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§ÙƒÙŠ", "ØªÙ…ÙˆÙŠÙ„ Ù…ØªÙ†Ø§Ù‡ÙŠ Ø§Ù„ØµØºØ±")
        pass
    
    def generate_from_user_logs(self, feedback_file: str) -> List[Dict]:
        """Extract high-quality questions from user feedback."""
        # Use questions that got positive feedback
        pass
```

#### Target Dataset Size

| Category | Count | Priority |
|----------|-------|----------|
| Factual questions | 40 | High |
| Comparison questions | 20 | High |
| Procedural questions | 15 | Medium |
| Conditional questions | 10 | High |
| Aggregation questions | 10 | Medium |
| Edge cases (not in docs) | 5 | High |
| **Total** | **100** | |

---

### 2. Evaluation Metrics Implementation

#### A. Retrieval Metrics

```python
# evaluation/retrieval_metrics.py

from typing import List, Dict
import numpy as np

class RetrievalEvaluator:
    """Evaluate retrieval quality."""
    
    def hit_rate_at_k(self, predictions: List[List[str]], ground_truth: List[List[str]], k: int = 5) -> float:
        """
        Percentage of queries where at least one relevant doc is in top-k.
        
        Args:
            predictions: List of retrieved chunk IDs for each query
            ground_truth: List of relevant chunk IDs for each query
        """
        hits = 0
        for pred, truth in zip(predictions, ground_truth):
            if any(p in truth for p in pred[:k]):
                hits += 1
        return hits / len(predictions)
    
    def mrr(self, predictions: List[List[str]], ground_truth: List[List[str]]) -> float:
        """
        Mean Reciprocal Rank - measures how high the first relevant result is.
        """
        rr_sum = 0
        for pred, truth in zip(predictions, ground_truth):
            for rank, p in enumerate(pred, 1):
                if p in truth:
                    rr_sum += 1 / rank
                    break
        return rr_sum / len(predictions)
    
    def precision_at_k(self, predictions: List[List[str]], ground_truth: List[List[str]], k: int = 5) -> float:
        """
        Percentage of retrieved docs that are relevant.
        """
        precisions = []
        for pred, truth in zip(predictions, ground_truth):
            relevant = sum(1 for p in pred[:k] if p in truth)
            precisions.append(relevant / k)
        return np.mean(precisions)
    
    def recall_at_k(self, predictions: List[List[str]], ground_truth: List[List[str]], k: int = 5) -> float:
        """
        Percentage of relevant docs that were retrieved.
        """
        recalls = []
        for pred, truth in zip(predictions, ground_truth):
            if len(truth) == 0:
                continue
            relevant = sum(1 for p in pred[:k] if p in truth)
            recalls.append(relevant / len(truth))
        return np.mean(recalls)
```

#### B. Generation Metrics (Ragas Integration)

```python
# evaluation/generation_metrics.py

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_correctness,
)
from datasets import Dataset

class GenerationEvaluator:
    """Evaluate LLM generation quality using Ragas."""
    
    def __init__(self, llm=None, embeddings=None):
        self.llm = llm
        self.embeddings = embeddings
    
    def evaluate_batch(self, data: List[Dict]) -> Dict[str, float]:
        """
        Evaluate a batch of predictions.
        
        Args:
            data: List of dicts with keys:
                - question: str
                - answer: str (system generated)
                - contexts: List[str] (retrieved chunks)
                - ground_truth: str (expected answer)
        """
        # Convert to Ragas dataset format
        dataset = Dataset.from_dict({
            'question': [d['question'] for d in data],
            'answer': [d['answer'] for d in data],
            'contexts': [d['contexts'] for d in data],
            'ground_truth': [d['ground_truth'] for d in data],
        })
        
        # Run evaluation
        results = evaluate(
            dataset=dataset,
            metrics=[
                faithfulness,       # Does answer stick to context?
                answer_relevancy,   # Does answer address the question?
                context_precision,  # Are retrieved contexts relevant?
                context_recall,     # Did we retrieve all needed context?
                answer_correctness, # Is the answer correct?
            ],
            llm=self.llm,
            embeddings=self.embeddings,
        )
        
        return {
            'faithfulness': results['faithfulness'],
            'answer_relevancy': results['answer_relevancy'],
            'context_precision': results['context_precision'],
            'context_recall': results['context_recall'],
            'answer_correctness': results['answer_correctness'],
        }
    
    def evaluate_single(self, question: str, answer: str, contexts: List[str], ground_truth: str) -> Dict[str, float]:
        """Evaluate a single prediction."""
        return self.evaluate_batch([{
            'question': question,
            'answer': answer,
            'contexts': contexts,
            'ground_truth': ground_truth,
        }])
```

#### C. Custom Arabic Metrics

```python
# evaluation/arabic_metrics.py

class ArabicEvaluator:
    """Arabic-specific evaluation metrics."""
    
    def citation_accuracy(self, answer: str, sources: List[Dict]) -> float:
        """
        Check if citations in answer match actual sources.
        """
        # Extract citation numbers from answer
        citations = re.findall(r'\[(\d+)\]', answer)
        
        # Check if each citation has corresponding source
        valid_citations = 0
        for cite_num in citations:
            idx = int(cite_num) - 1
            if idx < len(sources):
                # Check if cited text appears in source
                # ... fuzzy matching logic
                valid_citations += 1
        
        return valid_citations / max(len(citations), 1)
    
    def article_reference_accuracy(self, answer: str, ground_truth_articles: List[str]) -> float:
        """
        Check if answer references the correct articles/clauses.
        """
        # Extract article references from answer
        # e.g., "Ø§Ù„Ù…Ø§Ø¯Ø© 5", "Ø§Ù„Ø¨Ù†Ø¯ 3"
        found_articles = re.findall(r'(?:Ø§Ù„Ù…Ø§Ø¯Ø©|Ø§Ù„Ø¨Ù†Ø¯|Ø§Ù„ÙÙ‚Ø±Ø©)\s+(\d+)', answer)
        
        correct = sum(1 for a in found_articles if a in ground_truth_articles)
        return correct / max(len(ground_truth_articles), 1)
    
    def anti_hallucination_score(self, answer: str, contexts: List[str]) -> float:
        """
        Check if answer contains information not in contexts.
        Uses embedding similarity to detect potential hallucinations.
        """
        # Split answer into claims
        claims = self._extract_claims(answer)
        
        grounded_claims = 0
        for claim in claims:
            # Check if claim is supported by any context
            max_similarity = max(
                self._semantic_similarity(claim, ctx)
                for ctx in contexts
            )
            if max_similarity > 0.7:
                grounded_claims += 1
        
        return grounded_claims / max(len(claims), 1)
```

---

### 3. Evaluation Pipeline

```python
# evaluation/pipeline.py

class EvaluationPipeline:
    """End-to-end evaluation pipeline."""
    
    def __init__(self, rag_system, golden_dataset_path: str):
        self.system = rag_system
        self.dataset = self._load_dataset(golden_dataset_path)
        self.retrieval_evaluator = RetrievalEvaluator()
        self.generation_evaluator = GenerationEvaluator()
        self.arabic_evaluator = ArabicEvaluator()
    
    def run_full_evaluation(self) -> Dict[str, Any]:
        """Run complete evaluation and generate report."""
        
        # 1. Run system on all questions
        predictions = []
        for item in self.dataset:
            result = self.system.query_with_sources(item['question'])
            predictions.append({
                'question': item['question'],
                'answer': result['answer'],
                'contexts': [s['content'] for s in result['sources']],
                'retrieved_ids': [s['id'] for s in result['sources']],
                'ground_truth': item['ground_truth_answer'],
                'relevant_ids': item['relevant_chunks'],
            })
        
        # 2. Retrieval metrics
        retrieval_metrics = {
            'hit_rate@5': self.retrieval_evaluator.hit_rate_at_k(
                [p['retrieved_ids'] for p in predictions],
                [p['relevant_ids'] for p in predictions],
                k=5
            ),
            'mrr': self.retrieval_evaluator.mrr(
                [p['retrieved_ids'] for p in predictions],
                [p['relevant_ids'] for p in predictions]
            ),
            'precision@5': self.retrieval_evaluator.precision_at_k(
                [p['retrieved_ids'] for p in predictions],
                [p['relevant_ids'] for p in predictions],
                k=5
            ),
        }
        
        # 3. Generation metrics (Ragas)
        generation_metrics = self.generation_evaluator.evaluate_batch(predictions)
        
        # 4. Arabic-specific metrics
        arabic_metrics = {
            'citation_accuracy': np.mean([
                self.arabic_evaluator.citation_accuracy(p['answer'], p['contexts'])
                for p in predictions
            ]),
            'anti_hallucination': np.mean([
                self.arabic_evaluator.anti_hallucination_score(p['answer'], p['contexts'])
                for p in predictions
            ]),
        }
        
        # 5. Aggregate results
        return {
            'retrieval': retrieval_metrics,
            'generation': generation_metrics,
            'arabic': arabic_metrics,
            'overall_score': self._calculate_overall_score(
                retrieval_metrics, generation_metrics, arabic_metrics
            ),
            'detailed_results': predictions,
        }
    
    def generate_report(self, results: Dict) -> str:
        """Generate markdown evaluation report."""
        return f"""
# FRA RAG System Evaluation Report

## Date: {datetime.now().strftime('%Y-%m-%d')}

## Summary
- **Overall Score**: {results['overall_score']:.2%}

## Retrieval Metrics
| Metric | Score |
|--------|-------|
| Hit Rate@5 | {results['retrieval']['hit_rate@5']:.2%} |
| MRR | {results['retrieval']['mrr']:.3f} |
| Precision@5 | {results['retrieval']['precision@5']:.2%} |

## Generation Metrics
| Metric | Score |
|--------|-------|
| Faithfulness | {results['generation']['faithfulness']:.2%} |
| Answer Relevancy | {results['generation']['answer_relevancy']:.2%} |
| Context Precision | {results['generation']['context_precision']:.2%} |

## Arabic-Specific Metrics
| Metric | Score |
|--------|-------|
| Citation Accuracy | {results['arabic']['citation_accuracy']:.2%} |
| Anti-Hallucination | {results['arabic']['anti_hallucination']:.2%} |
        """
```

---

## Part 3: Implementation Roadmap

### Phase 1: Quick Wins (2-3 weeks)
| Feature | Effort | Impact | Priority |
|---------|--------|--------|----------|
| Clickable sources with highlighting | 1 week | High | âœ… DO FIRST |
| Semantic metadata extraction | 1-2 weeks | High | âœ… DO FIRST |
| Basic evaluation framework | 1 week | High | âœ… DO FIRST |
| New document ingestion | 1 day | Medium | âœ… DO NOW |

### Phase 2: Core Improvements (4-6 weeks)
| Feature | Effort | Impact | Priority |
|---------|--------|--------|----------|
| Hierarchical chunking (parent-document) | 2-3 weeks | Very High | âœ… |
| ReAct agent for multi-hop | 2-3 weeks | Very High | âœ… |
| Golden dataset creation (100 QA pairs) | 2 weeks | High | âœ… |

### Phase 3: Advanced Features (6-8 weeks)
| Feature | Effort | Impact | Priority |
|---------|--------|--------|----------|
| Knowledge Graph / GraphRAG | 4-6 weeks | High | âš ï¸ Complex |
| Full Ragas integration | 2 weeks | High | âœ… |
| Automated regression testing | 1 week | Medium | âœ… |

---

## Additional Enhancement Suggestions

### 1. Query Understanding Enhancement

```python
# Add query classification for better routing

class QueryClassifier:
    """Classify query intent for optimized handling."""
    
    INTENTS = {
        "definition": ["Ù…Ø§ Ù‡Ùˆ", "Ù…Ø§ Ø§Ù„Ù…Ù‚ØµÙˆØ¯", "Ø¹Ø±Ù"],
        "requirement": ["Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª", "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø´Ø±ÙˆØ·", "Ù…Ø§ ÙŠÙ„Ø²Ù…"],
        "procedure": ["ÙƒÙŠÙ", "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø·ÙˆØ§Øª", "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª"],
        "comparison": ["Ù…Ø§ Ø§Ù„ÙØ±Ù‚", "Ù‚Ø§Ø±Ù†", "Ø£ÙŠÙ‡Ù…Ø§"],
        "eligibility": ["Ù‡Ù„ ÙŠØ¬ÙˆØ²", "Ù‡Ù„ ÙŠØ­Ù‚", "Ù‡Ù„ ÙŠÙ…ÙƒÙ†"],
        "penalty": ["Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø©", "Ù…Ø§ Ù‡ÙŠ Ø§Ù„ØºØ±Ø§Ù…Ø©"],
        "timeline": ["Ù…ØªÙ‰", "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ø¯Ø©", "ÙƒÙ… ÙŠØ³ØªØºØ±Ù‚"],
    }
```

### 2. Confidence Scoring

```python
# Add confidence score to answers

def calculate_confidence(retrieval_scores: List[float], answer: str, contexts: List[str]) -> float:
    """
    Calculate confidence score for answer.
    
    Factors:
    1. Retrieval scores (are results highly relevant?)
    2. Citation density (is answer well-supported?)
    3. Context coverage (did we find enough context?)
    """
    retrieval_confidence = np.mean([s for s in retrieval_scores if s > 0.5])
    citation_count = len(re.findall(r'\[(\d+)\]', answer))
    coverage = min(citation_count / 3, 1.0)  # Expect at least 3 citations
    
    return (retrieval_confidence * 0.5 + coverage * 0.5)
```

### 3. Feedback-Driven Improvement

```python
# Use feedback to improve system

class FeedbackAnalyzer:
    """Analyze user feedback for improvement opportunities."""
    
    def identify_weak_topics(self, feedback_data: List[Dict]) -> List[str]:
        """Find topics with low satisfaction."""
        # Group by detected topic
        # Calculate negative feedback rate per topic
        # Return topics needing improvement
        pass
    
    def generate_improvement_suggestions(self) -> List[str]:
        """Generate actionable improvement suggestions."""
        pass
```

---

## Conclusion

This proposal outlines a comprehensive path to transform the FRA RAG system from a solid retrieval system into an advanced **Legal AI Assistant**. The key improvements are:

1. **Hierarchical Chunking**: Preserve legal document structure
2. **Semantic Metadata**: Enable powerful filtering
3. **ReAct Agent**: Multi-hop reasoning for complex questions
4. **Knowledge Graph**: Discover related concepts (future phase)
5. **Evaluation Framework**: Measure and improve systematically
6. **UI Enhancements**: Clickable sources with highlighting

**Recommended Starting Point**: 
1. Ingest new documents
2. Implement clickable sources (quick win)
3. Add metadata extraction
4. Set up basic evaluation
5. Implement hierarchical chunking
6. Add ReAct agent

Would you like me to begin implementing any of these features?
