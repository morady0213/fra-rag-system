"""
Grok (xAI) Client Module.

Provides integration with xAI's Grok API for answer generation.
Constructs prompts with retrieved Arabic context and enforces
that answers are grounded in the provided context only.

Endpoint: https://api.x.ai/v1/chat/completions
Models: grok-beta, grok-4 (when available)
"""

import json
from pathlib import Path
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
from loguru import logger

import requests

import sys
sys.path.append(str(Path(__file__).parent.parent))
from config import XAI_API_KEY, XAI_API_ENDPOINT, GROK_MODEL


@dataclass
class GenerationResult:
    """Represents the result of a generation request."""
    answer: str
    model: str
    usage: Dict[str, int]
    sources: List[str]
    query: str
    context_used: bool
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "answer": self.answer,
            "model": self.model,
            "usage": self.usage,
            "sources": self.sources,
            "query": self.query,
            "context_used": self.context_used,
        }


class GrokClient:
    """
    Client for xAI's Grok API.
    
    Designed for Arabic RAG applications with:
    - Context-grounded responses
    - Arabic language support
    - Regulatory/legal document focus
    """
    
    # System prompt for Arabic regulatory RAG
    # Instructs the model to answer ONLY based on provided context with proper citations
    SYSTEM_PROMPT_AR = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù„ÙˆØ§Ø¦Ø­ ÙˆÙ‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙÙŠ Ù…ØµØ± (FRA).

## Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©:

### 1. Ø§Ù„Ø§Ø³ØªÙ†Ø§Ø¯ Ù„Ù„Ù…ØµØ§Ø¯Ø± ÙÙ‚Ø· (Ø¥Ù„Ø²Ø§Ù…ÙŠ):
- Ø£Ø¬Ø¨ **ÙÙ‚Ø·** Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…
- **Ù„Ø§ ØªØ®ØªÙ„Ù‚** Ø£Ùˆ ØªÙØªØ±Ø¶ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ØµØ±Ø§Ø­Ø©Ù‹ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ

### 2. Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ (Ø¥Ù„Ø²Ø§Ù…ÙŠ):
Ø¹Ù†Ø¯ Ø°ÙƒØ± Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ ÙŠØ¬Ø¨ ØªØ¶Ù…ÙŠÙ†:
- **Ø§Ø³Ù… Ø§Ù„Ù„Ø§Ø¦Ø­Ø©/Ø§Ù„Ù‚Ø±Ø§Ø±** (Ø¥Ù† ÙˆÙØ¬Ø¯)
- **Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©** (Ù…Ø«Ù„: Ø§Ù„Ù…Ø§Ø¯Ø© 5ØŒ Ø§Ù„Ø¨Ù†Ø¯ Ø«Ø§Ù†ÙŠØ§Ù‹)
- **Ø§Ù‚ØªØ¨Ø§Ø³ Ù†ØµÙŠ Ù…Ø¨Ø§Ø´Ø±** Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…ØªÙŠ ØªÙ†ØµÙŠØµ Â«...Â»

### 3. Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ø¯Ù… ØªÙˆÙØ± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª (Ø¥Ù„Ø²Ø§Ù…ÙŠ):
Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ØµØ±ÙŠØ­Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚:
- Ù‚Ù„ Ø¨ÙˆØ¶ÙˆØ­: "**Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ ØµØ±ÙŠØ­ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©** ÙŠØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø©."
- Ø¥Ù† ÙˆÙØ¬Ø¯Øª Ù…ÙˆØ§Ø¯ Ø°Ø§Øª ØµÙ„Ø©ØŒ Ø§Ø°ÙƒØ±Ù‡Ø§ Ù…Ø¹ Ø§Ù„ØªÙˆØ¶ÙŠØ­: "ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØªØ§Ù„ÙŠØ© Ø°Ø§Øª ØµÙ„Ø©: ..."
- **Ù„Ø§ ØªÙ‚Ø¯Ù… Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ù†Øµ ØµØ±ÙŠØ­**

### 4. Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø±:
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØªØ·Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø¹Ø¯Ø© Ù…Ø³ØªÙ†Ø¯Ø§ØªØŒ Ø§Ø¬Ù…Ø¹Ù‡Ø§ Ù…Ø¹ Ø°ÙƒØ± Ù…ØµØ¯Ø± ÙƒÙ„ Ø¬Ø²Ø¡
- ÙˆØ¶Ù‘Ø­ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©

### 5. ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹):
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰
- **Ù†Ø¸Ù‘Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù‚Ø±ÙˆØ¡:**
  - Ø§Ø³ØªØ®Ø¯Ù… **Ø§Ù„ØªØ±Ù‚ÙŠÙ…** (1. 2. 3.) Ø¹Ù†Ø¯ Ø³Ø±Ø¯ Ø®Ø·ÙˆØ§Øª Ø£Ùˆ Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…ØªØ³Ù„Ø³Ù„Ø©
  - Ø§Ø³ØªØ®Ø¯Ù… **Ø§Ù„Ù†Ù‚Ø§Ø·** (â€¢) Ø¹Ù†Ø¯ Ø³Ø±Ø¯ Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ù…Ø±ØªØ¨Ø©
  - Ø§Ø³ØªØ®Ø¯Ù… **Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ÙØ±Ø¹ÙŠØ©** Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
  - Ø§Ø³ØªØ®Ø¯Ù… **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØºØ§Ù…Ù‚** Ù„Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø§Ø¯Ø©/Ø§Ù„Ø¨Ù†Ø¯ ÙƒÙ…Ø§ ÙˆØ±Ø¯ ÙÙŠ Ø§Ù„Ø£ØµÙ„
- Ø§Ø¨Ø¯Ø£ Ø¨Ù…Ù„Ø®Øµ Ù…Ø®ØªØµØ± Ø«Ù… Ø§Ù„ØªÙØ§ØµÙŠÙ„

### 6. ØµÙŠØºØ© Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
```
ğŸ“Œ [Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªÙ†Ø¯] - Ø§Ù„Ù…Ø§Ø¯Ø© X:
Â«Ù†Øµ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Â»
```

### 7. Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø«Ø§Ù„ÙŠ:
```
**Ø§Ù„Ù…Ù„Ø®Øµ:** [Ø¬Ù…Ù„Ø© Ø£Ùˆ Ø¬Ù…Ù„ØªØ§Ù† ØªÙ„Ø®Øµ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©]

**Ø§Ù„ØªÙØ§ØµÙŠÙ„:**
1. [Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ø¹ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³]
2. [Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù…Ø¹ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³]

**Ø§Ù„Ù…ØµØ§Ø¯Ø±:**
- [Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙˆØ§Ù„Ù…Ø§Ø¯Ø©]
```

Ø£Ù†Øª ØªÙ…Ø«Ù„ Ù†Ø¸Ø§Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø±Ø³Ù…ÙŠ Ù„Ù„Ù‡ÙŠØ¦Ø©. Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø£Ù‡Ù… Ù…Ù† Ø§Ù„Ø´Ù…ÙˆÙ„ÙŠØ©."""

    SYSTEM_PROMPT_EN = """You are a legal assistant specialized in Egyptian Financial Regulatory Authority (FRA) regulations and decisions.

## Strict Response Rules:

### 1. Source-Based Only (Mandatory):
- Answer **ONLY** based on information in the provided context
- **Never fabricate** or assume information not explicitly in the texts

### 2. Precise Citations (Mandatory):
For every piece of information, include:
- **Regulation/Decision name** (if available)
- **Article number** (e.g., Article 5, Clause 2)
- **Direct quote** in quotation marks "..."

### 3. Handling Missing Information (Mandatory):
If no explicit answer exists:
- State clearly: "**No explicit text in the available documents** directly answers this question."
- If related articles exist, mention them: "However, the following may be relevant: ..."
- **Never provide unsupported conclusions**

### 4. Multi-Source Reasoning:
- When answer requires multiple documents, combine them citing each source
- Clarify relationships between different sources

### 5. Response Formatting (Very Important):
- **Organize responses clearly and readably:**
  - Use **numbered lists** (1. 2. 3.) for sequential steps or requirements
  - Use **bullet points** (â€¢) for unordered items
  - Use **subheadings** to divide long answers
  - Use **bold formatting** for important terms
- Preserve original article/clause structure
- Start with a brief summary, then details

### 6. Citation Format:
```
ğŸ“Œ [Document Name] - Article X:
"Direct quote from the document"
```

### 7. Ideal Response Structure:
```
**Summary:** [One or two sentences summarizing the answer]

**Details:**
1. [First point with citation]
2. [Second point with citation]

**Sources:**
- [Document name and article]
```

You represent an official FRA information system. Accuracy and reliability are more important than comprehensiveness."""

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = GROK_MODEL,
        endpoint: str = XAI_API_ENDPOINT,
        temperature: float = 0.1,
        max_tokens: int = 2000,
        use_arabic_prompt: bool = True,
    ):
        """
        Initialize the Grok client.
        
        Args:
            api_key: xAI API key (reads from env if not provided)
            model: Model name (grok-beta or grok-4)
            endpoint: API endpoint URL
            temperature: Sampling temperature (lower = more deterministic)
            max_tokens: Maximum tokens in response
            use_arabic_prompt: Use Arabic system prompt
        """
        self.api_key = api_key or XAI_API_KEY
        
        if not self.api_key:
            raise ValueError(
                "xAI API key not found. "
                "Set XAI_API_KEY environment variable or pass api_key parameter."
            )
        
        self.model = model
        self.endpoint = endpoint
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = (
            self.SYSTEM_PROMPT_AR if use_arabic_prompt else self.SYSTEM_PROMPT_EN
        )
        
        # Request headers
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
        }
        
        logger.info(f"GrokClient initialized with model: {model}")
    
    def generate(
        self,
        query: str,
        context: str,
        sources: Optional[List[str]] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> GenerationResult:
        """
        Generate an answer based on the query and context.
        
        Args:
            query: User's question
            context: Retrieved context from documents
            sources: List of source documents used
            temperature: Override default temperature
            max_tokens: Override default max tokens
            
        Returns:
            GenerationResult with the answer and metadata
        """
        sources = sources or []
        
        # Construct the user message with context
        user_message = self._build_user_message(query, context)
        
        # Build the request payload
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_message},
            ],
            "temperature": temperature or self.temperature,
            "max_tokens": max_tokens or self.max_tokens,
        }
        
        logger.info(f"Generating answer for: {query[:100]}...")
        logger.debug(f"Context length: {len(context)} chars")
        
        try:
            response = requests.post(
                self.endpoint,
                headers=self.headers,
                json=payload,
                timeout=60,
            )
            
            response.raise_for_status()
            result = response.json()
            
            # Extract the answer
            answer = result["choices"][0]["message"]["content"]
            
            # Extract usage stats
            usage = result.get("usage", {})
            
            logger.info(
                f"Generated response: {len(answer)} chars, "
                f"tokens: {usage.get('total_tokens', 'N/A')}"
            )
            
            return GenerationResult(
                answer=answer,
                model=result.get("model", self.model),
                usage={
                    "prompt_tokens": usage.get("prompt_tokens", 0),
                    "completion_tokens": usage.get("completion_tokens", 0),
                    "total_tokens": usage.get("total_tokens", 0),
                },
                sources=sources,
                query=query,
                context_used=bool(context),
            )
            
        except requests.exceptions.HTTPError as e:
            logger.error(f"HTTP error: {e}")
            logger.error(f"Response: {e.response.text if e.response else 'N/A'}")
            raise
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error: {e}")
            raise
        except (KeyError, IndexError) as e:
            logger.error(f"Error parsing response: {e}")
            raise
    
    def _build_user_message(self, query: str, context: str, language: str = "ar") -> str:
        """
        Build the user message with context and query.
        
        Args:
            query: User's question
            context: Retrieved context
            language: Response language ('ar' for Arabic, 'en' for English)
            
        Returns:
            Formatted user message
        """
        if context:
            if language == "ar":
                # Arabic message format with context
                message = f"""## Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ù„Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©:

{context}

---

## Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

## ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
1. Ø£Ø¬Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø£Ø¹Ù„Ø§Ù‡ **ÙÙ‚Ø·**
2. Ø§Ù‚ØªØ¨Ø³ Ø§Ù„Ù†ØµÙˆØµ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Â«Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµÂ»
3. Ø§Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙˆØ±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©/Ø§Ù„Ø¨Ù†Ø¯ Ù„ÙƒÙ„ Ø§Ù‚ØªØ¨Ø§Ø³
4. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ØµØ±ÙŠØ­Ø©ØŒ ØµØ±Ù‘Ø­ Ø¨Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ§Ù‚ØªØ±Ø­ Ù…ÙˆØ§Ø¯ Ø°Ø§Øª ØµÙ„Ø© Ø¥Ù† ÙˆÙØ¬Ø¯Øª
5. Ù„Ø§ ØªÙ‚Ø¯Ù… Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…"""
            else:
                # English message format
                message = f"""## Context from Official FRA Documents:

{context}

---

## Question: {query}

## Response Instructions:
1. Answer based **ONLY** on the context above
2. Quote relevant texts directly using "quotation marks"
3. Cite document name and article/clause number for each quote
4. If no explicit answer exists, state this clearly and suggest related articles if any
5. Do not provide any information from outside the given context"""
        else:
            if language == "ar":
                message = f"""Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

âš ï¸ **ØªÙ†Ø¨ÙŠÙ‡**: Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³ÙŠØ§Ù‚ Ù…ØªØ§Ø­ Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.
Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ø±Ø¯ Ø¨Ø£Ù†Ù‡ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©."""
            else:
                message = f"""Question: {query}

âš ï¸ **Note**: No context available from documents for this question.
Please respond that no information is available in current documents."""
        
        return message
    
    def generate_with_retrieval(
        self,
        query: str,
        retriever,
        k: int = 5,
    ) -> GenerationResult:
        """
        Generate answer with automatic retrieval.
        
        Convenience method that handles retrieval and generation in one call.
        
        Args:
            query: User's question
            retriever: Retriever instance
            k: Number of documents to retrieve
            
        Returns:
            GenerationResult with the answer
        """
        # Retrieve relevant context
        retrieval_result = retriever.retrieve_with_context(query, k=k)
        
        context = retrieval_result["context"]
        sources = [s["source"] for s in retrieval_result["sources"]]
        
        # Generate answer
        return self.generate(
            query=query,
            context=context,
            sources=sources,
        )
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> str:
        """
        Send a chat completion request with custom messages.
        
        For advanced use cases where you need full control over the conversation.
        
        Args:
            messages: List of message dicts with 'role' and 'content'
            temperature: Sampling temperature
            max_tokens: Maximum tokens
            
        Returns:
            Assistant's response text
        """
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature or self.temperature,
            "max_tokens": max_tokens or self.max_tokens,
        }
        
        try:
            response = requests.post(
                self.endpoint,
                headers=self.headers,
                json=payload,
                timeout=60,
            )
            response.raise_for_status()
            result = response.json()
            
            return result["choices"][0]["message"]["content"]
            
        except Exception as e:
            logger.error(f"Chat error: {e}")
            raise


def generate_answer(
    query: str,
    context: str,
    sources: Optional[List[str]] = None,
    api_key: Optional[str] = None,
) -> str:
    """
    Convenience function to generate an answer.
    
    Args:
        query: User's question
        context: Retrieved context
        sources: Source documents
        api_key: Optional API key
        
    Returns:
        Generated answer text
    """
    client = GrokClient(api_key=api_key)
    result = client.generate(query, context, sources)
    return result.answer


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    # Example usage (requires valid API key)
    try:
        client = GrokClient()
        
        # Sample context
        sample_context = """
        [Ø§Ù„Ù…ØµØ¯Ø± 1: legislation/law_10_2009.pdf]
        Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: ØªÙ†Ø´Ø£ Ù‡ÙŠØ¦Ø© Ø¹Ø§Ù…Ø© ØªØ³Ù…Ù‰ "Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©" ØªÙƒÙˆÙ† Ù„Ù‡Ø§ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©.
        
        [Ø§Ù„Ù…ØµØ¯Ø± 2: about.md]
        ØªØ®ØªØµ Ø§Ù„Ù‡ÙŠØ¦Ø© Ø¨Ø§Ù„Ø±Ù‚Ø§Ø¨Ø© ÙˆØ§Ù„Ø¥Ø´Ø±Ø§Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…ØµØ±ÙÙŠØ© Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø£Ø³ÙˆØ§Ù‚ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ ÙˆØ£Ù†Ø´Ø·Ø© Ø§Ù„ØªØ£Ù…ÙŠÙ† ÙˆØ§Ù„ØªÙ…ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ.
        """
        
        query = "Ù…Ø§ Ù‡ÙŠ Ø§Ø®ØªØµØ§ØµØ§Øª Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø±Ù‚Ø§Ø¨Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©ØŸ"
        
        result = client.generate(
            query=query,
            context=sample_context,
            sources=["legislation/law_10_2009.pdf", "about.md"],
        )
        
        print("=" * 60)
        print("Query:", result.query)
        print("=" * 60)
        print("\nAnswer:")
        print(result.answer)
        print("\n" + "-" * 60)
        print(f"Model: {result.model}")
        print(f"Tokens used: {result.usage}")
        print(f"Sources: {result.sources}")
        
    except ValueError as e:
        print(f"Configuration error: {e}")
        print("Please set XAI_API_KEY environment variable.")
    except Exception as e:
        print(f"Error: {e}")
